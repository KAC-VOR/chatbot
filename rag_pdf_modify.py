# -*- coding: utf-8 -*-
# ì´ íŒŒì¼ì€ Streamlit ê¸°ë°˜ RAG(ê²€ìƒ‰ ì¦ê°• ìƒì„±) ì•±ì˜ í•µì‹¬ ì½”ë“œì…ë‹ˆë‹¤.
# ì£¼ì˜: ì‹¤ì œ ìš´ì˜ ì‹œì—” API í‚¤ ê°™ì€ ë¯¼ê°ì •ë³´ë¥¼ ì ˆëŒ€ë¡œ í•˜ë“œì½”ë”©í•˜ì§€ ë§ê³ , ë³¸ ì½”ë“œì²˜ëŸ¼ st.secrets ë“±ì„ ì‚¬ìš©í•˜ì„¸ìš”.

import os  # ìš´ì˜ì²´ì œ ê¸°ëŠ¥(í™˜ê²½ë³€ìˆ˜, ê²½ë¡œ ë“±)ì„ ì‚¬ìš©í•˜ê¸° ìœ„í•œ í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬
import tempfile  # ì„ì‹œ íŒŒì¼ì„ ë§Œë“¤ê³  ê´€ë¦¬í•˜ê¸° ìœ„í•œ í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬

import streamlit as st  # ê°„ë‹¨íˆ ì›¹ì•±ì„ ë§Œë“¤ ìˆ˜ ìˆëŠ” í”„ë ˆì„ì›Œí¬(ì‚¬ì´ë“œë°”, ì…ë ¥ì°½, ì¶œë ¥ ë“± UI ì œê³µ)
from langchain.document_loaders import PyPDFLoader  # PDF íŒŒì¼ì„ ë¬¸ì„œ ê°ì²´ë“¤ë¡œ ë¶ˆëŸ¬ì˜¤ëŠ” ë„êµ¬
from langchain.text_splitter import RecursiveCharacterTextSplitter  # ê¸´ í…ìŠ¤íŠ¸ë¥¼ ì ë‹¹í•œ í¬ê¸° ë©ì–´ë¦¬(ì²­í¬)ë¡œ ìë¥´ëŠ” ë„êµ¬
from langchain_core.runnables import RunnablePassthrough  # ì²´ì¸ ì‹¤í–‰ ì‹œ ì…ë ¥ì„ ê·¸ëŒ€ë¡œ ë‹¤ìŒ ë‹¨ê³„ì— ì „ë‹¬í•˜ëŠ” ìœ í‹¸ (ì—¬ê¸°ì„  ì§ì ‘ ì‚¬ìš©í•˜ì§„ ì•ŠìŒ)
from langchain_openai import OpenAIEmbeddings, ChatOpenAI  # OpenAI ì„ë² ë”©/ì±— ëª¨ë¸ì„ LangChainì—ì„œ ì“°ê¸° ìœ„í•œ ë˜í¼
from langchain_community.vectorstores import Chroma  # í…ìŠ¤íŠ¸ ì„ë² ë”©ì„ ì €ì¥/ê²€ìƒ‰í•˜ëŠ” ë²¡í„°DB(Chroma) ì—°ë™ ë˜í¼
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder  # LLMì— ë³´ë‚¼ í”„ë¡¬í”„íŠ¸(ì§ˆë¬¸ í˜•ì‹) í…œí”Œë¦¿ êµ¬ì„± ë„êµ¬
from langchain.chains.combine_documents import create_stuff_documents_chain  # ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ë‹µë³€ í”„ë¡¬í”„íŠ¸ì— "ê·¸ëƒ¥ ë¼ì›Œ ë„£ëŠ”" ì²´ì¸ ìƒì„± í•¨ìˆ˜
from langchain.chains import create_history_aware_retriever, create_retrieval_chain  # ëŒ€í™”ì´ë ¥ ì¸ì§€ ê²€ìƒ‰ê¸°/ìµœì¢… RAG ì²´ì¸ ìƒì„± í•¨ìˆ˜
from langchain_core.runnables.history import RunnableWithMessageHistory  # ì²´ì¸ì— ëŒ€í™” ì´ë ¥(ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬)ì„ ë¶™ì—¬ ìƒíƒœ ìœ ì§€í•˜ê²Œ í•˜ëŠ” ë„êµ¬
from langchain_community.chat_message_histories.streamlit import StreamlitChatMessageHistory  # Streamlit ì„¸ì…˜ì— ëŒ€í™” ì´ë ¥ì„ ì €ì¥/ë¡œë“œí•˜ëŠ” êµ¬í˜„ì²´
from langchain_core.output_parsers import StrOutputParser  # ëª¨ë¸ ì‘ë‹µì„ ë¬¸ìì—´ë¡œ íŒŒì‹±í•˜ëŠ” ìœ í‹¸ (ì—¬ê¸°ì„  ì§ì ‘ ì‚¬ìš©í•˜ì§„ ì•ŠìŒ)
from openai import OpenAI  # OpenAI ê³µì‹ SDK(ì§ì ‘ Chat Completions APIë¥¼ í˜¸ì¶œí•  ë•Œ ì‚¬ìš©)

# from streamlit import caching  # (ì£¼ì„ ì²˜ë¦¬) Streamlitì˜ êµ¬ ìºì‹œ API; í˜„ì¬ëŠ” st.cache_* ê¶Œì¥

# ê±°ì˜ ìµœì¢…ë²„ì „ì´ë¼ëŠ” ë©”ëª¨. ì‹¤ì œë¡  ì§€ì†ì ìœ¼ë¡œ ê°œì„ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
################### ë°°í¬ ë•Œë¬¸ì— ì¶”ê°€ ###################
### ì•„ë˜ ì´ìŠˆëŠ” Streamlit Cloud/ì¼ë¶€ í™˜ê²½ì—ì„œ sqlite3 ë²„ì „ ë¶ˆì¼ì¹˜ë¡œ Chromaê°€ ì˜¤ë¥˜ë‚  ë•Œì˜ ìš°íšŒì±…ì…ë‹ˆë‹¤.
### ì°¸ê³  í† ë¡ : https://discuss.streamlit.io/t/chromadb-sqlite3-your-system-has-an-unsupported-version-of-sqlite3/90975

import pysqlite3  # íŒŒì´ì¬ìš© SQLite ëŒ€ì²´ ëª¨ë“ˆ(ë‚´ì¥ sqlite3 ëŒ€ì‹  ì‚¬ìš©)
import sys  # íŒŒì´ì¬ ì¸í„°í”„ë¦¬í„° ë° ëª¨ë“ˆ ì‹œìŠ¤í…œ ì ‘ê·¼ìš© í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬
import sqlite3  # ë‚´ì¥ sqlite3 ëª¨ë“ˆ(ì•„ë˜ì—ì„œ ëŒ€ì²´ë¨)
sys.modules["sqlite3"] = sys.modules.pop("pysqlite3")  # íŒŒì´ì¬ì´ sqlite3ë¥¼ ë¶ˆëŸ¬ì˜¤ë ¤ í•  ë•Œ pysqlite3ë¥¼ ëŒ€ì‹  ì“°ê²Œë” ë°”ê¿‰ë‹ˆë‹¤.

#########################
# ì˜¤í”ˆAI API í‚¤ ì„¤ì • (ì ˆëŒ€ ì½”ë“œì— ì§ì ‘ í‚¤ë¥¼ ì“°ì§€ ë§ê³ , Streamlit Secretsë¥¼ ì“°ì„¸ìš”!)
os.environ["OPENAI_API_KEY"] = st.secrets['OPENAI_API_KEY']  # OpenAI ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì°¸ì¡°í•  í™˜ê²½ë³€ìˆ˜ì— í‚¤ë¥¼ ì£¼ì…
id = st.secrets['id']  # ë§¤ìš° ë‹¨ìˆœí•œ ë¡œê·¸ì¸ ê²€ì‚¬ìš© ë¹„ë°€ê°’(ì‹¤ì„œë¹„ìŠ¤ì—ì„  OAuth ë“± ì•ˆì „í•œ ë°©ì‹ ê¶Œì¥)


def cache_clear():  # Streamlitì˜ ë¦¬ì†ŒìŠ¤ ìºì‹œë¥¼ ì§€ìš°ëŠ” í•¨ìˆ˜ (ë³€ê²½ì‚¬í•­ ë°˜ì˜/ì„¸ì…˜ ì´ˆê¸°í™”ê°€ í•„ìš”í•  ë•Œ ì‚¬ìš©)
    # st.cache_data.clear()  # ë°ì´í„° ìºì‹œë¥¼ ë¹„ìš°ëŠ” ì½”ë“œ(í˜„ì¬ëŠ” ì‚¬ìš© ì•ˆ í•¨)
    st.cache_resource.clear()  # ë¦¬ì†ŒìŠ¤ ìºì‹œ(ëª¨ë¸, DB ì—°ê²° ë“±)ë¥¼ ë¹„ì›ë‹ˆë‹¤.
    # caching.clear_cache()  # êµ¬ë²„ì „ ìºì‹œ ì´ˆê¸°í™” ì½”ë“œ(í˜„ì¬ ì£¼ì„)


# ì•„ë˜ ë°ì½”ë ˆì´í„°ëŠ” í•¨ìˆ˜ ê²°ê³¼ë¥¼ ì„¸ì…˜ ë™ì•ˆ ì¬ì‚¬ìš©(ìºì‹±)í•´ì„œ ì†ë„ í–¥ìƒì„ ë•ìŠµë‹ˆë‹¤.
@st.cache_resource
def load_and_split_pdf(file_path):  # íŒŒì¼ ê²½ë¡œë¡œ PDFë¥¼ ë¡œë“œí•´ í˜ì´ì§€ ë‹¨ìœ„ ë¬¸ì„œë“¤ë¡œ ë¶„ë¦¬í•˜ëŠ” í•¨ìˆ˜
    loader = PyPDFLoader(file_path)  # PDF ë¡œë” ì¤€ë¹„
    return loader.load_and_split()  # PDFë¥¼ ë¡œë“œí•˜ê³  í˜ì´ì§€ë³„ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜


# í…ìŠ¤íŠ¸ ì²­í¬ë“¤ì„ Chroma ì•ˆì— ì„ë² ë”© ë²¡í„°ë¡œ ì €ì¥í•˜ëŠ” ì „ ë‹¨ê³„: PDF ì—…ë¡œë“œ ì²˜ë¦¬
@st.cache_resource  # ì—…ë¡œë“œ/íŒŒì‹± ê²°ê³¼ë¥¼ ìºì‹œí•´ ê°™ì€ íŒŒì¼ë¡œ ì¬ì‹¤í–‰ ì‹œ ì†ë„ë¥¼ ë†’ì…ë‹ˆë‹¤.
def load_pdf(_file):  # Streamlit íŒŒì¼ ì—…ë¡œë”ê°€ ì¤€ íŒŒì¼ê°ì²´ë¥¼ ë°›ì•„ ì²˜ë¦¬

    with tempfile.NamedTemporaryFile(mode="wb", delete=False) as tmp_file:  # OSê°€ ì„ì‹œ íŒŒì¼ì„ ìƒì„±(ë‚˜ì¤‘ì— ìš°ë¦¬ê°€ ì§€ìš¸ ìˆ˜ ìˆë„ë¡ delete=False)
        tmp_file.write(_file.getvalue())  # ì—…ë¡œë“œëœ íŒŒì¼ì˜ ë°”ì´íŠ¸ë¥¼ ì„ì‹œ íŒŒì¼ì— ê¸°ë¡
        tmp_file_path = tmp_file.name  # ì„ì‹œ íŒŒì¼ ê²½ë¡œ ë¬¸ìì—´ì„ í™•ë³´
        # PDF íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ -> ì´ì œ ë¡œë”©
        loader = PyPDFLoader(file_path=tmp_file_path)  # ì„ì‹œ ê²½ë¡œë¥¼ ì‚¬ìš©í•´ ë¡œë” ìƒì„±
        pages = loader.load_and_split()  # í˜ì´ì§€ë³„ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ë¡œ ë¶„í•´
    return pages  # í˜ì´ì§€ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜


@st.cache_resource
def create_vector_store(_docs):  # ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°›ì•„ ë²¡í„°DB(Chroma)ë¥¼ ë©”ëª¨ë¦¬ ìƒì—ì„œ ìƒì„±
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)  # 500ì ë‹¨ìœ„ë¡œ, 50ì ê²¹ì¹˜ê²Œ ë¶„í• 
    split_docs = text_splitter.split_documents(_docs)  # ë¬¸ì„œë“¤ì„ ì‘ì€ ì²­í¬ë“¤ë¡œ ë‚˜ëˆ”
    # persist_directory = "./chroma_pdf_db"  # ë””ìŠ¤í¬ì— ì˜êµ¬ì €ì¥í•˜ë ¤ë©´ ê²½ë¡œë¥¼ ì„¤ì •(ì—¬ê¸°ì„  ë©”ëª¨ë¦¬ ì‚¬ìš©ì´ë¼ ì£¼ì„)
    vectorstore = Chroma.from_documents(  # ë¶„í• ëœ ë¬¸ì„œë¥¼ ì„ë² ë”©í•˜ì—¬ Chromaì— ì €ì¥(ë©”ëª¨ë¦¬)
        split_docs,
        OpenAIEmbeddings(model='text-embedding-3-small'),  # ê°€ë²¼ìš´ ì„ë² ë”© ëª¨ë¸ ì„ íƒ(ë¹„ìš©/ì†ë„/ì •í™•ë„ ê· í˜•)
        # persist_directory=persist_directory  # ë””ìŠ¤í¬ì— ì €ì¥í•˜ë ¤ë©´ ì£¼ì„ í•´ì œ
    )
    return vectorstore  # ê²€ìƒ‰(retrieval)ì— ì‚¬ìš©í•  ë²¡í„° ìŠ¤í† ì–´ ë°˜í™˜


# (ì˜µì…˜) ì´ë¯¸ ë””ìŠ¤í¬ì— ë§Œë“¤ì–´ë‘” ChromaDBë¥¼ ì¬ì‚¬ìš©í•˜ë ¤ëŠ” ê²½ìš° ë¶ˆëŸ¬ì˜¤ëŠ” í•¨ìˆ˜
@st.cache_resource
def get_vectorstore():  # ì˜êµ¬ì €ì¥ëœ Chroma ì¸ìŠ¤í„´ìŠ¤ë¥¼ ê²½ë¡œì—ì„œ ë¡œë“œ
    persist_directory = "./chroma_db"  # ë¯¸ë¦¬ ë§Œë“¤ì–´ ë‘” DB í´ë” ê²½ë¡œ(í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê¸°ì¤€)
    print(persist_directory)  # ë””ë²„ê·¸ìš© ì¶œë ¥(ì„œë²„ ë¡œê·¸ì—ì„œ í™•ì¸ ê°€ëŠ¥)
    # if os.path.exists(persist_directory):  # ê²½ë¡œ í™•ì¸ ë¡œì§(í˜„ì¬ëŠ” ë‹¨ìˆœí™”ë¥¼ ìœ„í•´ ì£¼ì„ ì²˜ë¦¬)
    return Chroma(  # í•´ë‹¹ ê²½ë¡œì˜ DBë¥¼ ë¡œë“œ
        persist_directory=persist_directory,
        embedding_function=OpenAIEmbeddings(model='text-embedding-3-small')  # ë™ì¼í•œ ì„ë² ë”© ì„¤ì • í•„ìš”
    )
    # else:
    #     return 0  # (ë‹¨ìˆœí™”ë¥¼ ìœ„í•´ ë¯¸ì‚¬ìš©)


def format_docs(docs):  # ê²€ìƒ‰ëœ ì—¬ëŸ¬ ë¬¸ì„œë¥¼ í•˜ë‚˜ì˜ í° ë¬¸ìì—´ë¡œ í•©ì¹˜ëŠ” ê°„ë‹¨í•œ ìœ í‹¸
    return "\n\n".join(doc.page_content for doc in docs)  # ë¬¸ì„œë“¤ì˜ ë³¸ë¬¸ì„ ë‘ ì¤„ ê³µë°±ìœ¼ë¡œ ì´ì–´ë¶™ì„


# PDF ë¬¸ì„œ ë¡œë“œ-ë²¡í„° DB-ê²€ìƒ‰ê¸°-ëŒ€í™” ì´ë ¥ê¹Œì§€ ëª¨ë‘ í•©ì¹œ RAG ì²´ì¸ ì´ˆê¸°í™” í•¨ìˆ˜(ê¸°ì¡´ DB ì¬ì‚¬ìš© ëª¨ë“œ)
@st.cache_resource
def initialize_components(selected_model):  # ì„ íƒí•œ LLM ëª¨ë¸ëª…ì„ ë°›ì•„ ì²´ì¸ì„ êµ¬ì„±
    # file_path = r"../data/"  # (ì˜ˆì‹œ ê²½ë¡œ) ì§ì ‘ íŒŒì¼ ë¡œë”©ì´ í•„ìš”í•˜ë©´ ì‚¬ìš©
    # file_path = r"C:/Users/Jay/PycharmProjects/test_ai/input3.pdf"  # (ì˜ˆì‹œ ê²½ë¡œ)
    # pages = load_and_split_pdf(file_path)  # (ì˜ˆì‹œ) íŒŒì¼ì—ì„œ ì§ì ‘ ë¡œë“œí•˜ë ¤ë©´ ì‚¬ìš©
    vectorstore = get_vectorstore()  # ë””ìŠ¤í¬ì˜ Chroma DBë¥¼ ë¡œë“œ
    retriever = vectorstore.as_retriever()  # ë²¡í„° DBë¥¼ ì§ˆì˜ìš© ê²€ìƒ‰ê¸°ë¡œ ë³€í™˜

    # ì‚¬ìš©ìì˜ ìµœì‹  ì§ˆë¬¸ì´ ê³¼ê±° ëŒ€í™” ë§¥ë½ì„ ê°€ë¦¬í‚¬ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ìš°ì„  'ë…ë¦½í˜• ì§ˆë¬¸'ìœ¼ë¡œ ì¬êµ¬ì„±í•˜ëŠ” ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
    contextualize_q_system_prompt = """Given a chat history and the latest user question \
    which might reference context in the chat history, formulate a standalone question \
    which can be understood without the chat history. Do NOT answer the question, \
    just reformulate it if needed and otherwise return it as is."""  # ì˜ì–´ ìœ ì§€: ëª¨ë¸ í”„ë¡¬í”„íŠ¸ ê´€ë¡€

    contextualize_q_prompt = ChatPromptTemplate.from_messages(  # ìœ„ ì‹œìŠ¤í…œ ì§€ì‹œ + ê³¼ê±°ì´ë ¥ + í˜„ì¬ ì…ë ¥ì„ í•˜ë‚˜ì˜ ë©”ì‹œì§€ ì„¸íŠ¸ë¡œ ë¬¶ìŒ
        [
            ("system", contextualize_q_system_prompt),  # ì‹œìŠ¤í…œ ì—­í•  ë©”ì‹œì§€(ê·œì¹™/ì§€ì‹œ)
            MessagesPlaceholder("history"),  # ê³¼ê±° ëŒ€í™” ì´ë ¥ì´ ì—¬ê¸°ì— ì±„ì›Œì§
            ("human", "{input}"),  # ì‚¬ìš©ìì˜ í˜„ì¬ ì§ˆë¬¸(í¬ë§·íŒ…ìœ¼ë¡œ ì£¼ì…)
        ]
    )

    # ì‹¤ì œ ë‹µë³€ìš© ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸(ê²€ìƒ‰ëœ ë¬¸ë§¥ ê¸°ë°˜ìœ¼ë¡œë§Œ ë‹µí•˜ë„ë¡ ì—„ê²©íˆ ì§€ì‹œ)
    qa_system_prompt = """You are an assistant for question-answering tasks. \
    Use the following pieces of retrieved context to answer the question. \
    Please say answer only based pieces of retrieved context.\
    If you don't know the answers, Please say you don't know. \
    Keep the answer perfect. please use imogi with the answer.\
    ëŒ€ë‹µì€ í•œêµ­ì–´ë¡œ í•˜ê³ , ì¡´ëŒ“ë§ì„ ì¨ì¤˜.\
    {context}"""  # í•œêµ­ì–´ë¡œ ê³µì†í•˜ê²Œ ë‹µí•˜ê³ , ì´ëª¨ì§€ë„ ì“°ë¼ëŠ” ì§€ì‹œ í¬í•¨

    qa_prompt = ChatPromptTemplate.from_messages(  # ìµœì¢… ë‹µë³€ í”„ë¡¬í”„íŠ¸ êµ¬ì„±(ì‹œìŠ¤í…œ+ì´ë ¥+í˜„ì¬ ì§ˆë¬¸)
        [
            ("system", qa_system_prompt),
            MessagesPlaceholder("history"),
            ("human", "{input}"),
        ]
    )

    # í† í°ì œí•œ ê±±ì • ë©”ëª¨(ì‹¤ì œ ì œì–´ëŠ” model, chunking, kê°’ ë“±ìœ¼ë¡œ ì¡°ì ˆ)
    llm = ChatOpenAI(model=selected_model, temperature=0)  # OpenAI ì±— ëª¨ë¸ì„ 0ì˜¨ë„(ì°½ì˜ì„± ìµœì†Œ)ë¡œ ì„¤ì •: ë¬¸ì„œ ê¸°ë°˜ ì‚¬ì‹¤ìœ„ì£¼ ë‹µë³€ ìœ ë„

    # ëŒ€í™” ì´ë ¥ì„ ê³ ë ¤í•´, ë¨¼ì € ì§ˆë¬¸ì„ ë…ë¦½í˜•ìœ¼ë¡œ ê°€ë‹¤ë“¬ê³ (retriever), ê·¸ ì§ˆë¬¸ìœ¼ë¡œ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ëŠ” ê²€ìƒ‰ê¸° êµ¬ì„±
    history_aware_retriever = create_history_aware_retriever(llm, retriever, contextualize_q_prompt)
    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)  # ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ í”„ë¡¬í”„íŠ¸ì— "ë¶™ì—¬ë„£ì–´" ë‹µë³€í•˜ëŠ” ì²´ì¸
    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)  # ê²€ìƒ‰ê³¼ ë‹µë³€ ì²´ì¸ì„ í•œë° ë¬¶ì€ ìµœì¢… ì²´ì¸

    return rag_chain  # UIì—ì„œ ì´ ì²´ì¸ì„ í˜¸ì¶œí•˜ì—¬ ì§ˆì˜ì‘ë‹µì„ ìˆ˜í–‰


# ì•„ë˜ëŠ” ë‹¤ì–‘í•œ ê²€ìƒ‰ ì „ëµ ì˜ˆì‹œ(í˜„ì¬ ë¯¸ì‚¬ìš©). í•„ìš” ì‹œ ì£¼ì„ í•´ì œí•˜ì—¬ ì‹¤í—˜.
# retriever = vectorstore.as_retriever(
#     search_type="mmr", search_kwargs={"k": 2, "fetch_k": 10, "lambda_mult": 0.6})


# ChatGPT ë‹¨ë… ëª¨ë“œ(ë‚´ ìë£Œ ê²€ìƒ‰ ì—†ì´)ë¡œ ê°„ë‹¨ ëŒ€í™”í•˜ëŠ” í™”ë©´ ë¡œì§
# @st.cache_resource  # (ìºì‹œ ë¶ˆí•„ìš”í•´ì„œ ì£¼ì„)
def initial_not_select(selected_model):  # ëª¨ë¸ëª…ì„ ë°›ì•„ ChatGPT ëŒ€í™”ë§Œ í•˜ëŠ” ëª¨ë“œ
    if "messages" not in st.session_state:  # ì„¸ì…˜ì— ëŒ€í™” ê¸°ë¡ì´ ì—†ë‹¤ë©´
        st.session_state["messages"] = [{"role": "assistant", "content": "ì•ˆë…•í•˜ì„¸ìš” ì±—ë´‡ì…ë‹ˆë‹¤. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"}]  # ì²« ì¸ì‚¬ ì €ì¥
    print(st.session_state)  # ë””ë²„ê·¸ìš© ì¶œë ¥
    for msg in st.session_state.messages:  # ì§€ê¸ˆê¹Œì§€ì˜ ëŒ€í™”ë¥¼ í™”ë©´ì— ìˆœì„œëŒ€ë¡œ ì¶œë ¥
        st.chat_message(msg["role"]).write(msg["content"])  # ì—­í• ì— ë”°ë¼ ë§í’ì„  í˜•íƒœë¡œ í‘œì‹œ
    # prompt = ""  # (ì‚¬ìš© ì•ˆ í•¨)
    if prompt := st.chat_input(key=1):  # í•˜ë‹¨ ì…ë ¥ì°½ì—ì„œ ì‚¬ìš©ìê°€ ì§ˆë¬¸ì„ ì…ë ¥í•˜ë©´
        client = OpenAI()  # OpenAI ê³µì‹ SDK í´ë¼ì´ì–¸íŠ¸ ìƒì„±
        st.session_state.messages.append({"role": "user", "content": prompt})  # ëŒ€í™” ê¸°ë¡ì— ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        st.chat_message("user").write(prompt)  # ì‚¬ìš©ì ë§í’ì„  ì¶œë ¥
        response = client.chat.completions.create(model=selected_model, messages=st.session_state.messages)  # Chat Completions API í˜¸ì¶œ
        msg = response.choices[0].message.content  # ëª¨ë¸ì˜ ë‹µë³€ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
        st.session_state.messages.append({"role": "assistant", "content": msg})  # ê¸°ë¡ì— ë‹µë³€ ì¶”ê°€
        st.chat_message("assistant").write(msg)  # í™”ë©´ì— ë‹µë³€ ì¶œë ¥


@st.cache_resource
def chaining(_pages, selected_model):  # ì—…ë¡œë“œí•œ PDFë¥¼ ì¦‰ì„ì—ì„œ ë²¡í„°DBë¡œ ë§Œë“¤ì–´ ì‚¬ìš©í•˜ëŠ” ëª¨ë“œ
    vectorstore = create_vector_store(_pages)  # ë¬¸ì„œì—ì„œ ë²¡í„° ìŠ¤í† ì–´ ìƒì„±(ë©”ëª¨ë¦¬)
    retriever = vectorstore.as_retriever()  # ê²€ìƒ‰ê¸°ë¡œ ë³€í™˜
    # retriever = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 4})  # ê°™ì€ ì˜ë¯¸(ì˜ˆì‹œ)

    # ìµœì‹  ì§ˆë¬¸ì„ ë…ë¦½í˜•ìœ¼ë¡œ ì¬êµ¬ì„±í•˜ê¸° ìœ„í•œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸(ìœ„ì™€ ë™ì¼ ê°œë…)
    contextualize_q_system_prompt = """Given a chat history and the latest user question \
       which might reference context in the chat history, formulate a standalone question \
       which can be understood without the chat history. Do NOT answer the question, \
       just reformulate it if needed and otherwise return it as is."""

    contextualize_q_prompt = ChatPromptTemplate.from_messages(  # ì‹œìŠ¤í…œ ì§€ì‹œ + ê³¼ê±°ì´ë ¥ + í˜„ì¬ ì…ë ¥ êµ¬ì„±
        [
            ("system", contextualize_q_system_prompt),
            MessagesPlaceholder("history"),
            ("human", "{input}"),
        ]
    )

    # ì´ ë¶€ë¶„ì˜ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ëŠ” íŒ€ ì·¨í–¥ëŒ€ë¡œ ë°”ê¿€ ìˆ˜ ìˆìŠµë‹ˆë‹¤(ì¡´ëŒ“ë§/ì´ëª¨ì§€ ì§€ì‹œ ë“± í¬í•¨).
    qa_system_prompt = """You are an assistant for question-answering tasks. \
    Use the following pieces of retrieved context to answer the question. \
    Please say answer only based pieces of retrieved context.\
    If you don't know the answers, Please say you don't know. \
    Keep the answer perfect. please use imogi with the answer.\
    ëŒ€ë‹µì€ í•œêµ­ì–´ë¡œ í•˜ê³ , ì¡´ëŒ“ë§ì„ ì¨ì¤˜.\
    {context}"""

    qa_prompt = ChatPromptTemplate.from_messages(  # ìµœì¢… ë‹µë³€ í”„ë¡¬í”„íŠ¸(ì—¬ê¸°ì„  history ì—†ì´ ê°„ë‹¨ êµ¬ì„±)
        [
            ("system", qa_system_prompt),
            ("human", "{input}"),
        ]
    )

    llm = ChatOpenAI(model=selected_model, temperature=0)  # ì‚¬ì‹¤ ìœ„ì£¼ ì‘ë‹µì„ ìœ„í•´ temperature=0
    history_aware_retriever = create_history_aware_retriever(llm, retriever, contextualize_q_prompt)  # ëŒ€í™” ì´ë ¥ ì¸ì§€ ê²€ìƒ‰ê¸°
    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)  # ë¬¸ì„œë¥¼ í†µì§¸ë¡œ í”„ë¡¬í”„íŠ¸ì— ê²°í•©í•˜ëŠ” ì²´ì¸
    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)  # ê²€ìƒ‰+ìƒì„± ê²°í•©
    return rag_chain  # í˜¸ì¶œ ì¸¡ì—ì„œ invokeë¡œ ì‹¤í–‰


# ì„¸ì…˜ ìƒíƒœì— ë¡œê·¸ì¸ í”Œë˜ê·¸ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ Falseë¡œ ì‹œì‘
if "logged_in" not in st.session_state:
    st.session_state["logged_in"] = False  # ì•„ì§ ë¡œê·¸ì¸ ì•ˆ ë¨


# ë§¤ìš° ë‹¨ìˆœí•œ ë¡œê·¸ì¸ í•¨ìˆ˜(ì‹¤ì„œë¹„ìŠ¤ ìš©ë„ê°€ ì•„ë‹Œ ë°ëª¨ ìˆ˜ì¤€). ì…ë ¥ê°’ì´ secretsì˜ idì™€ ê°™ìœ¼ë©´ í†µê³¼.
def login(id_in):  # ì‚¬ìš©ìê°€ ì…ë ¥í•œ ì•„ì´ë””(id_in)ë¥¼ ë°›ì•„ ê²€ì‚¬
    if id == id_in:  # ë¯¸ë¦¬ ì €ì¥í•´ ë‘” ì‹œí¬ë¦¿ê³¼ ë™ì¼í•˜ë©´
        st.session_state["logged_in"] = True  # ë¡œê·¸ì¸ ì„±ê³µ í”Œë˜ê·¸ ì„¤ì •
        st.success("login succes")  # ì„±ê³µ ë©”ì‹œì§€(ì˜¤íƒ€ëŠ” ìœ ì§€)
        st.rerun()  # í˜„ì¬ ì•±ì„ ì¬ì‹¤í–‰í•˜ì—¬ ë¡œê·¸ì¸ ì´í›„ í™”ë©´ì„ ì¦‰ì‹œ ë³´ì—¬ì¤Œ
    else:
        st.error(("Wrong ID and Password"))  # ì‹¤íŒ¨ ë©”ì‹œì§€(ë¹„ë°€ë²ˆí˜¸ëŠ” ì‹¤ì œë¡  ê²€ì¦í•˜ì§€ ì•ŠìŒ)


# ë¡œê·¸ì¸ ì—¬ë¶€ì— ë”°ë¼ ë‹¤ë¥¸ í™”ë©´ì„ ë Œë”ë§
if not st.session_state["logged_in"]:  # ì•„ì§ ë¡œê·¸ì¸ ì „ì´ë©´
    st.title("Login1")  # ìƒë‹¨ ì œëª© í‘œì‹œ
    id_in = st.text_input("ID")  # ì•„ì´ë”” ì…ë ¥ì°½
    pw = st.text_input("password", type="password")  # ë¹„ë°€ë²ˆí˜¸ ì…ë ¥ì°½(ê°€ë ¤ì„œ í‘œì‹œ). ì‹¤ì œ ê²€ì¦ì€ ì•ˆ í•¨
    if st.button("Login"):  # ë¡œê·¸ì¸ ë²„íŠ¼ì„ ëˆ„ë¥´ë©´
        cache_clear()  # ìºì‹œë¥¼ ë¹„ì›Œ ê¹”ë”í•œ ìƒíƒœë¡œ ì‹œì‘
        print(id_in)  # ë””ë²„ê·¸ìš© ì¶œë ¥
        print(id)  # ë””ë²„ê·¸ìš© ì¶œë ¥(ìš´ì˜ì—ì„  ë¯¼ê°ì •ë³´ ë¡œê·¸ì¶œë ¥ ê¸ˆì§€!)
        login(id_in)  # ë¡œê·¸ì¸ ì‹œë„
else:

        # Streamlit UI (ë¡œê·¸ì¸ ì´í›„ ë³´ì—¬ì¤„ ë³¸ ê¸°ëŠ¥ í™”ë©´)
    st.header("í•­ê³µí†µì‹ ì†Œ Q&A ì±—ë´‡ ğŸ’¬")  # ì•±ì˜ ë©”ì¸ í—¤ë”
    selection = st.selectbox("ChatGpt,ê¸°ì¡´ Database(ë…¸í•˜ìš° ë“±), PDF ", ("ChatGpt", "Database", "PDF"))  # ì‚¬ìš©í•  ëª¨ë“œ ì„ íƒ
    option = st.selectbox("Select GPT Model", ("gpt-4.1-mini", "gpt-4.1"))  # ì‚¬ìš©í•  OpenAI ëª¨ë¸ ì„ íƒ
    # st.slider('ëª‡ì‚´ì¸ê°€ìš”?', 0, 130, 25)  # (ì˜ˆì‹œ UI) í˜„ì¬ ë¯¸ì‚¬ìš©
    # halu_t = st.slider("ê¸°ì¡´ ë¬¸ì„œë¡œ ë‹µë³€: 0, ì°½ì˜ë ¥ ì¶”ê°€ ë‹µë³€: 1", 0.0,1.0,(0.0))  # (ì˜ˆì‹œ)
    # halu = st.selectbox("ê¸°ì¡´ ë¬¸ì„œë¡œ ë‹µë³€: 0, ì°½ì˜ë ¥ ì¶”ê°€ ë‹µë³€: 1",("0","0.5","1"))  # (ì˜ˆì‹œ)

    # halu = str(halu_t)  # (ì˜ˆì‹œ)

    # print(halu_t)  # (ì˜ˆì‹œ)
    if selection == "ChatGpt":  # ChatGPT ë‹¨ë… ëª¨ë“œ ì„ íƒ ì‹œ
        cache_clear()  # ì´ì „ ë¦¬ì†ŒìŠ¤ ìºì‹œë¥¼ ì§€ì›Œ ëª¨ë“œ ì „í™˜ ì‹œ ì¶©ëŒ ë°©ì§€
        initial_not_select(option)  # ë‹¨ë… ëŒ€í™” UI ì‹¤í–‰

    if selection == "PDF":  # PDF ê¸°ë°˜ RAG ëª¨ë“œ ì„ íƒ ì‹œ
        # cache_clear()  # (í•„ìš”ì— ë”°ë¼ ìºì‹œ ì´ˆê¸°í™”)
        uploaded_file = st.file_uploader("PDF ê¸°ë°˜ ë‹µë³€", type=["pdf"], accept_multiple_files=True)  # ì—¬ëŸ¬ PDF ì—…ë¡œë“œ
        for file in uploaded_file:  # ì—…ë¡œë“œëœ ê° íŒŒì¼ì— ëŒ€í•´
            pages = load_pdf(file)  # ì„ì‹œíŒŒì¼ë¡œ ì €ì¥ í›„ í˜ì´ì§€ ë¦¬ìŠ¤íŠ¸ ë¡œë“œ
            print(pages)  # ë””ë²„ê·¸ìš©(í˜ì´ì§€ ê°ì²´ ëª©ë¡)
            print(type(pages))  # ë””ë²„ê·¸ìš©(ìë£Œí˜•)
        try:
            rag_chain = chaining(pages, option)  # ë°©ê¸ˆ ë¡œë“œí•œ í˜ì´ì§€ë“¤ë¡œ ë²¡í„°DBë¥¼ ë§Œë“¤ê³  RAG ì²´ì¸ êµ¬ì„±
            # print(rag_chain)  # ë””ë²„ê·¸ìš©
            chat_history = StreamlitChatMessageHistory(key="chat_messages")  # ì„¸ì…˜ì— ëŒ€í™” ì´ë ¥ì„ ë³´ê´€/ë³µì›í•˜ëŠ” ê°ì²´
            if "messages" not in st.session_state:  # (ë³„ë„ ëŒ€í™” ê¸°ë¡ í‚¤ë¥¼ ì“°ëŠ” ê²½ìš° ì´ˆê¸°í™”)
                st.session_state["messages"] = [{"role": "assistant",
                                                 "content": "ë¬´ì—‡ì´ë“  ë¬¼ì–´!"}]  # ê°„ë‹¨í•œ ì•ˆë‚´ ë§í’ì„ 

            conversational_rag_chain = RunnableWithMessageHistory(  # ì²´ì¸ì— "ëŒ€í™” ì´ë ¥" ê¸°ëŠ¥ì„ ì—°ê²°í•´, ë§¤ í„´ íˆìŠ¤í† ë¦¬ê°€ ì „ë‹¬ë˜ê²Œ í•¨
                rag_chain,
                lambda session_id: chat_history,  # ì„¸ì…˜IDë¡œ íˆìŠ¤í† ë¦¬ ê°ì²´ë¥¼ ë°˜í™˜í•˜ëŠ” ì½œë°±(ì—¬ê¸°ì„  ê³ ì •)
                input_messages_key="input",  # ì²´ì¸ì— ì…ë ¥ í”„ë¡¬í”„íŠ¸ê°€ ë“¤ì–´ê°ˆ í‚¤ ì´ë¦„
                history_messages_key="history",  # ê³¼ê±° ëŒ€í™”ê°€ ì£¼ì…ë  í‚¤ ì´ë¦„
                output_messages_key="answer",  # ì²´ì¸ ê²°ê³¼ì—ì„œ ë‹µë³€ì´ ë‹´ê¸°ëŠ” í‚¤ ì´ë¦„
            )

            for msg in chat_history.messages:  # ì§€ê¸ˆê¹Œì§€ì˜ íˆìŠ¤í† ë¦¬ë¥¼ UIì— í‘œì‹œ
                st.chat_message(msg.type).write(msg.content)

            if prompt_message := st.chat_input("Your question"):  # ì‚¬ìš©ì ì§ˆë¬¸ ì…ë ¥
                st.chat_message("human").write(prompt_message)  # ì‚¬ìš©ì ë§í’ì„  í‘œì‹œ
                with st.chat_message("ai"):  # AI ë§í’ì„  ì»¨í…ìŠ¤íŠ¸ ë‚´ë¶€ì—ì„œ
                    with st.spinner("Thinking..."):  # ì²˜ë¦¬ ì¤‘ ìŠ¤í”¼ë„ˆ í‘œì‹œ
                        config = {"configurable": {"session_id": "any"}}  # íˆìŠ¤í† ë¦¬ ì‹ë³„ìš© ì„¸ì…˜ID(ê°„ë‹¨íˆ ê³ ì • ë¬¸ìì—´ ì‚¬ìš©)
                        response = conversational_rag_chain.invoke(  # ì²´ì¸ì„ í•œ ë²ˆ ì‹¤í–‰í•˜ì—¬ ê²°ê³¼ ë°›ê¸°
                            {"input": prompt_message},
                            config)

                        answer = response['answer']  # ì²´ì¸ì´ ë°˜í™˜í•œ ë”•ì…”ë„ˆë¦¬ì—ì„œ ë‹µë³€ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
                        st.write(answer)  # ë‹µë³€ ì¶œë ¥
                        # with st.expander("ì°¸ê³  ë¬¸ì„œ í™•ì¸"):  # ì°¸ê³ í•œ ë¬¸ì„œ(ê·¼ê±°)ë„ ë³´ì—¬ì£¼ê³  ì‹¶ë‹¤ë©´ ì£¼ì„ í•´ì œ
                        #     for doc in response['context']:
                        #         st.markdown(doc.metadata['source'], help=doc.page_content)
        except:  # ì—…ë¡œë“œ/íŒŒì‹±/ì²´ì¸ êµ¬ì„± ì¤‘ ì˜¤ë¥˜ê°€ ë‚˜ë©´(ì˜ˆ: íŒŒì¼ ë¯¸ì„ íƒ)
            st.header("ğŸ“š PDF ì—…ë¡œë“œ í•´ì£¼ì„¸ìš”!")  # ì‚¬ìš©ìì—ê²Œ ì—…ë¡œë“œ ìš”ì²­ ì•ˆë‚´
            # st.header("í•œë²ˆì— ì—¬ëŸ¬ íŒŒì¼ ì—…ë¡œë“œ")  # (ì¶”ê°€ ì•ˆë‚´ ì˜ˆì‹œ)

    elif selection == "Database":  # ê¸°ì¡´ì— êµ¬ì¶•í•´ ë‘” ë¡œì»¬ Chroma DBë¡œ ë‹µë³€í•˜ëŠ” ëª¨ë“œ
        # cache_clear()  # (í•„ìš”ì— ë”°ë¼ ìºì‹œ ì´ˆê¸°í™”)
        rag_chain = initialize_components(option)  # DB ë¡œë“œ + RAG ì²´ì¸ ì¤€ë¹„
        chat_history = StreamlitChatMessageHistory(key="chat_messages")  # ëŒ€í™” ì´ë ¥ ê´€ë¦¬ ê°ì²´

        conversational_rag_chain = RunnableWithMessageHistory(  # íˆìŠ¤í† ë¦¬ í¬í•¨ ì²´ì¸ ë˜í¼
            rag_chain,
            lambda session_id: chat_history,
            input_messages_key="input",
            history_messages_key="history",
            output_messages_key="answer",
        )
        print(st.session_state)  # ë””ë²„ê·¸ìš© ì¶œë ¥
        if "messages" not in st.session_state:  # ì—†ìœ¼ë©´ ê°„ë‹¨í•œ ì•ˆë‚´ ë©”ì‹œì§€ë¡œ ì´ˆê¸°í™”
            st.session_state["messages"] = [{"role": "assistant",
                                             "content": "ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”!"}]

        for msg in chat_history.messages:  # ì´ì „ ëŒ€í™” í‘œì‹œ
            st.chat_message(msg.type).write(msg.content)

        if prompt_message := st.chat_input("Your question"):  # ì§ˆë¬¸ ì…ë ¥
            st.chat_message("human").write(prompt_message)  # ì‚¬ìš©ì ë§í’ì„ 
            with st.chat_message("ai"):  # AI ë§í’ì„ 
                with st.spinner("Thinking..."):  # ì²˜ë¦¬ ì¤‘ í‘œì‹œ
                    config = {"configurable": {"session_id": "any"}}  # ì„¸ì…˜ID ì„¤ì •(ê°„ë‹¨ ê³ ì •)
                    response = conversational_rag_chain.invoke(  # ì²´ì¸ ì‹¤í–‰
                        {"input": prompt_message},
                        config)

                    answer = response['answer']  # ë‹µë³€ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
                    st.write(answer)  # í™”ë©´ ì¶œë ¥
                    # with st.expander("ì°¸ê³  ë¬¸ì„œ í™•ì¸"):  # ê·¼ê±° ë¬¸ì„œ UI (ì›í•˜ë©´ ì‚¬ìš©)
                    #     for doc in response['context']:
                    #         st.markdown(doc.metadata['source'], help=doc.page_content)  # ë¬¸ì„œ ì¶œì²˜/ë‚´ìš© íŒíŠ¸
